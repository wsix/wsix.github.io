---
title: "【机器学习】 一些重要概念总结"
categories:
  - technology
tags:
  - machine learning
---

最近在准备面试的过程中发现自己有很多知识并没有真正的掌握，正好趁着这个时候对之前学习过的一些机器学习的算法整理一下，做一下记录。学然后知不足，在机器学习的学习中不仅要知其然还要知其所以然，这一点在大公司的面试中尤为重要。接下来的一段时间我需要好好地将之前不扎实的地方好好夯实，以备接下来的一大波笔试面试。

**声明**：文章中涉及到的概念主要来自南京大学周志华老师的**《机器学习》**，还有部分概念搜集自网络。

## 判别式与生成式模型

> <font size="2">Andrew Ng在NIPS2001年有一篇专门比较判别模型和产生式模型的文章, 具体可见http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf </font>

<font size="2">下表是关于判别式模型与生成式模型的一些比较，转自http://blog.163.com/huai_jing@126/blog/static/1718619832011227757554/</font>

<table>
<thead>
<tr>
<th align="center" width="12%"></th>
<th align="left" width="44%">判别式模型（ discriminative model ）</th>
<th align="left" width="44%">产生式模型（ generative model ）</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">特点</td>
<td align="left">寻找不同类别之间的最优分类面，反映的是异类数据之间的差异</td>
<td align="left">对后验概率建模，从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度</td>
</tr>
<tr>
<td align="center">区别(输入 x, 标签 y)</td>
<td align="left">估计的是条件概率分布 (conditional distribution) : $ P(y | x) $</td>
<td align="left">估计的是联合概率分布（joint probability distribution) : $ P(x, y) $</td>
</tr>
<tr>
<td align="center">联系</td>
<td align="left" colspan="2">由产生式模型可以得到判别式模型，但由判别式模型得不到产生式模型。</td>
</tr>
<tr>
<td align="center">常见模型</td>
<td align="left">- 支持向量机 <br> - 传统的神经网络 <br> - 线性判别分析 <br> - 线性回归 <br> - Boosting 集成模型 <br> - 条件随机场 <br> - 近邻学习 </td>
<td align="left">- 朴素贝叶斯 <br> - 混合多项式 <br> - 混合高斯模型以及其他混合模型 <br> - 混合专家模型 <br> - 隐马尔可夫模型 <br> - 马尔可夫的随机场 <br> - 文档主题生成模型(LDA) </td>
</tr>
<tr>
<td align="center">优点</td>
<td align="left">1 ）分类边界更灵活，比使用纯概率方法或产生式模型更高级；<br> 2 ）能清晰的分辨出多类或某一类与其他类之间的差异特征；<br> 3 ）在聚类、 viewpoint changes, partial occlusion and scale variations 中的效果较好；<br> 4 ）适用于较多类别的识别；<br> 5 ）判别模型的性能比产生式模型要简单，比较容易学习。</td>
<td align="left">1 ）实际上带的信息要比判别模型丰富； <br> 2 ）研究单类问题比判别模型灵活性强； <br> 3 ）模型可以通过增量学习得到； <br> 4 ）能用于数据不完整（ missing data ）情况。</td>
</tr>
<tr>
<td align="center">缺点</td>
<td align="left">1 ）不能反映训练数据本身的特性。能力有限，可以告诉你的是 1 还是 2 ，但没有办法把整个场景描述出来； <br> 2 ） Lack elegance of generative: Priors, 结构 ,  不确定性； <br> 3 ） Alternative notions of penalty functions, regularization,  核函数； <br> 4 ）黑盒操作 :  变量间的关系不清楚，不可视。</td>
<td align="left">1 ) Tend to produce a significant number of false positives. This is particularly true for object classes which share a high visual similarity such as horses and cows ； <br> 2 )  学习和计算过程比较复杂。</td>
</tr>
<tr>
<td align="center">性能</td>
<td align="left">较好（性能比生成模型稍好些，因为利用了训练数据的类别标识信息，缺点是不能反映训练数据本身的特性）</td>
<td align="left">较差</td>
</tr>
<tr>
<td align="center">主要应用</td>
<td align="left">Image and document classification <br>Biosequence analysis <br>Time series prediction</td>
<td align="left">NLP(natural language processing ) <br> Medical Diagnosis</td>
</tr>
</tbody>
</table>

## 线性回归与 LR

### 优缺点

LR 的优点：

1. 实现简单；
2. 分类时计算量非常小，速度很快，存储资源低，容易并行；（在大量数据下这个模型非常适用）。
3. 处理分类问题的同时还可能给出一个概率值；
4. 可以选用的优化方法多；除了 GD 和 SGD，还可以使用牛顿法、BFGS 和 L-BFGS。

LR 的缺点：

1. 容易欠拟合，一般准确度不太高
2. 只能处理两分类问题（在此基础上衍生出来的 softmax 可以用于多分类），必须线性可分。

## 神经网络

## 决策树

### ID3 决策树

> 算法流程
> 1. 针对当前的集合，计算每个特征的信息增益；
> 2. 选择信息增益最大的特征作为当前节点的划分选择；
> 3. 根据第 2 步选定的划分特征的可能值划分不同的子节点（比如年龄特征有青年，中年，老年，则划分三个子节点）；
> 4. 对子节点重复 1 - 3 步骤，直到所有特征都被划分完毕。

“信息熵” 是度量样本集合纯度最常用的一种指标。假定当前样本集合 D 中第 k 类样本所占的比例为 $ \{ p\_k (k = 1, 2, \cdots, \|y\|) \} $，则 D 的信息熵为

$$
Ent(D) = - \sum_{k=1}^{|y|}{p_k\log_2{p_k}}
$$

此时我们便可以定义信息增益了：

$$
Gain(D, a) = Ent(D) - \sum_{v=1}^V{\frac{|D^v|}{|D|}Ent(D^v)}
$$

**损失函数**

设决策树的叶子节点个数为 $ \{ T \} $，$ \{ t \} $ 为其中一个叶子节点，该节点有 $ \{ N\_t \} $ 个样本，其中 $ \{ k \} $ 类的样本有 $ \{ N\_{tk} \} $ 个，$ \{ Ent（t） \} $ 为叶子节点的信息熵，那么决策树的损失函数定义为

$$
C_t(T) = \sum_t{(N_t \times Ent(t))} + \lambda|T|
$$

将上面信息熵的公式代入可得

$$
C_t(T) = \sum_t{\sum_k{(N_{tk} \times \log_2{\frac{N_{tk}}{N_t}})}} + \lambda|T|
$$

$ \{ \lambda\|T\| \} $ 是正则化项，其中 $ \{ \lambda \} $ 用于调节比率

### C4.5 决策树

是 ID3 的一个改进算法，使用信息增益率来进行属性的选择。增益率定义如下

$$
Gain_ratio(D, a) = \frac{Gain(D,a)}{IV(a)}
$$

其中，

$$
IV(a) = -\sum_{v=1}^V{\frac{|D_v|}{|D|}\log_2{\frac{|D_v|}{|D|}}}
$$

增益率与信息增益的比较：

- 信息增益对可取数据较多的属性有所偏好，增益率对可取数值较少的属性有所偏好；
- 增益率准确率高，但是子树的构造过程中需要进行多次扫描和排序，所以它的运算效率较低。

### CART

分类回归树（Classification and Regression Tree）是一个决策二叉树，在构建的过程中，每个节点在分裂的时候都是通过最好的方式将剩余的样本分为两类。划分指标按照分类和回归有所不同：

- 分类树：基尼指数最小化
- 回归树：平方误差最小化

基尼值反映了从数据集中随机抽取两个样本，它们类别不一致的概率。

$$
Gini(D) = 1 - \sum_{k=1}^{|y|}{p_k^2}
$$

属性 a 的基尼指数定义为

$$
Gini_index(D,a) = \sum_{v = 1}^{V}{\frac{|D^v|}{|D|}Gini(D^v)}
$$

### 决策树的选择

1. 离散特征值 + 离散目标值：可以使用 ID3 或者 CART
2. 连续特征值 + 离散目标值：将连续特征离散化之后使用 ID3 或者 CART

分类树：输出叶子节点所属类别最多的那一类；
回归树：输出叶子节点中各个样本值的平均值。

**理想的决策树**

1. 叶子节点尽量少
2. 树的高度尽量小

### 解决决策树的过拟合问题

1. 剪枝
    - 预剪枝：在分裂节点之前检查划分前后的泛化性能，如果泛化性能降低则停止剪枝。（可能会造成欠拟合）
    - 后剪枝：在决策树建立之后，对删除节点前后的泛化性能进行检测，如果删除节点能够提高泛化性能，则进行剪枝。（训练开销大）
2. 交叉验证
3. 随机森林

### 优缺点

决策树的优点：

1. 计算简单，可解释性强，比较适合处理有缺失属性的样本，能够处理不相关的特征；

决策树的缺点：

1. 单棵决策树分类能力弱，对于连续值变量不易处理；
2. 容易过拟合。

## 支持向量机

### 优缺点

SVM 优点：

1. 分类思想简单，就是将样本与决策面的间隔最大化；
2. 分类效果较好；
3. 训练结束，大部分样本都不需要保留，最终模型只与支持向量有关；
4. 使用核函数可以向高维空间进行映射，解决非线性分类。

SVM 缺点：

1. 对大规模数据训练比较困难；
2. 无法直接支持多分类，但是可以使用间接的方法来做。

### SMO 算法

求解 SVM 对偶问题是一个二次规划问题，可以使用二次规划算法来求解；但是，使用这种方法往往会给任务造成很大的开销。此时可以使用 SMO 算法快速求解 SVM。

它选择凸二次规划的两个变量，其他变量保持不变，然后根据这两个变量构建一个二次规划问题，这个二次规划关于这两个变量的解会较之前更加接近问题的真实解，通过这样的子问题划分可以大大提高整个算法的计算速度，这两个变量可以按照如下的原则选择：

1. 首先选择严重违反 KKT 条件的一个变量；
2. 接着选择与使目标函数值增长最快的变量。

### SVM 多分类问题

- 直接法：直接在目标函数上进行修改，将多个分类面的参数求解合并到一个最优化问题中，通过求解该优化就可以实现多分类（计算复杂度很高，实现起来较为困难）；
- 间接法：
    1. One Vs All
    2. One Vs One

## K 近邻

### 优缺点

KNN 的优点：

1. 思想简单，理论成熟，既可以用来做分类也可以用来做回归；
2. 可用于非线性分类；
3. 训练时间复杂度为 O(n)；
4. 准确度高，对数据没有假设，对 outlier 不敏感。

KNN 的缺点：

1. 计算量大；
2. 样本不平衡时准确度无法保证问题；
3. 需要大量内存；

### KD 树

KD 树是一个二叉树，表示对 K 维空间的一个划分，可以进行快速检索（这样 KNN 在计算的时候就不需要对全样本进行距离的计算了）。

**构造 KD 树** 是一个在 K 维的空间上循环找子区域的中位数进行划分的过程。假设现在有 K 维空间的数据集 $ \{T = \\{ x\_1, x\_2, \cdots, x\_n \\} \} $,$ \{ x\_i = \\{a\_1,a\_2, \cdots, a\_k \\} \} $，可以按照如下步骤构造 KD 树。

1. 首先构造根节点，以 $ \{ a\_1 \} $ 的中位数 $ \{ b\_1 \} $ 为划分点，将根节点对应的矩阵区域划分为两个区域，区域 1 中 $ \{ a\_1 < b\_1 \} $，区域 2 中 $ \{ a\_1 > b\_1 \} $；
2. 构造叶子节点，选择 $ \{ a\_2 \} $ 属性，对于之前划分好的区域，以 $ \{ a\_2 \} $ 属性的中位数 $ \{ b\_2 \} $ 作为 划分点，再次将它们两两划分，作为划分前区域的叶子节点（如果 $ \{ a\_2 \} $ = $ \{ b\_2 \} $，那么 $ \{ a\_2 \} $ 的实例落在切分面），此时的这些刚划分好的子区域称为深度为 1 的叶子节点；
3. 不断重复第 2 步，深度为 j 的叶子节点划分的时候，使用的属性 $ \{ a\_i \} $ 的 $ \{ i = j \mod k + 1 \} $，直到划分之后的子区域没有实例时停止。

**KD 树的搜索**

1. 首先从根节点开始递归往下查找包含 $ \{ x \} $ 的叶子节点，每一层都是找对应的 $ \{ a_i \} $；
2. 将这个叶子节点认为是当前的“近似最近点”；
3. 递归向上回退，如果以 $ \{ x \} $ 为圆心，以“近似最近点”为半径的球与根节点的另一半子区域边界相交，则说明另一半子区域中存在与 $ \{ x \} $ 更近的点，则进入另一个子区域中查找该点并更新“近似最近点”；
4. 重复步骤 3，直到另一子区域与球体不想交或者退回根节点
5. 最后更新的“近似最近点”即是与 $ \{ x \} $ 真正最近的点。

**KD 树进行 KNN 查找**

通过 KD 树的搜索找到与搜索目标最近的点，这样 KNN 的搜索就可以被限制在空间的局部区域上了，可以大大增加查找效率。

**KD 树搜索的复杂度**

当实例随机分布的时候，搜索的时间复杂度为 $ \{ O(log(n)) \} $，n 为实例的个数，KD 树更加适用于实例数量远大于空间维度的 KNN 搜索，如果实例的空间维度与实例个数差不多时，它的效率基本等于线性查找。

## 集成学习

### 随机森林

> 学习过程
> 1. 现在有 N 个训练样本，每个样本的特征为 M 个， 需要建 K 棵决策树；
> 2. 从 N 个训练样本中有放回的取 N 个样本作为一组训练集（其余未取到的样本作为预测分类，评估其包外误差）
> 3. 从 M 个特征中取 m 个特征（m << M），从这些特征中选择一个最优特征用于对节点的划分
> 4. 对采样的数据使用完全分裂的方式建立决策树，这样的决策树每个节点要么无法分裂，要么所有的样本都指向同一个分类
> 5. 重复 2-4 K 次，即完成随机森林的构建

**预测过程**

1. 将预测样本分别输入到 K 棵树分别进行预测；
2. 如果是分类问题，直接使用投票方式选出票数最高的类别；
3. 如果是回归问题，使用各个树输出的均值作为结果。

**参数问题**

1. 一般取 $ \{m = \sqrt{M}\} $
2. 树的个数 K，跟具体的样本有关（比如特征数量）
3. 树的最大深度，（太深可能会导致泛化性能下降）
4. 节点上的最小样本数，最小信息增益（防止过拟合）

**泛化误差估计**

和 Bagging 一样采用 oob(out-of-bag) 进行泛化误差的估计，将各个树的未采样样本作为预测样本（约为36.8%），使用已经建立好的森林（不包含使用这些样本训练的决策树）对各个预测样本集进行预测。预测完之后统计误分的个数占总预测样本的比率作为 RF 的 oob 估计。

**优缺点**

1. 能够处理大量特征的分类，并且避免了特征选择
2. 在训练完成之后可以看出哪些 feature 比较重要
3. 训练速度很快
4. 容易实现，并且很容易并行训练
5. 效果很好

### GBDT

> GBDT的精髓在于训练的时候都是以上一课树的残差为目标，这个残差就是上一个树的预测值与真实值的差值。

<font size="2"> 比如，当前样本年龄为 18 岁，那么第一棵树会按照目标值为 18 岁来进行训练，但是训练完之后预测的年龄为 12 岁，差值为 6。接下来，第二棵树就会以 6 岁为目标值进行训练，假如训练之后预测出来的结果是 6，那么两个树累加起来就是真实年龄了。如果预测出来结果是 5，那么剩余的残差 1 就会交给第三棵树去训练，以此类推。 </font>

机器学习中的学习算法的目标是为了优化或者说最小化loss Function， Gradient boosting的思想是迭代生多个（M个）弱的模型，然后将每个弱模型的预测结果相加，后面的模型$ \{ F\_{m+1}(x) \} $基于前面学习模型的$ \{ F\_{m+1}(x) \} $的效果生成的，如下所示：

$$
F_{m+1}(x) = F_m(x) + h(x),  1 \leq m \leq M
$$

其中 $ \{ h(x) \} $ 就是我们在这个过程中训练得到的决策树，下面就是如何生成这个模型的问题了，很容易能想到新生成的 $ \{ F\_{m+1}(x) \} $ 应该尽可能的与真实标记 $ \{ y \} $ 接近，此时也就是说根据残差进行学习。残差学习在回归问题中可以很好的使用，但是为了一般性（分类，排序问题），实际中往往是基于loss Function 在函数空间的的负梯度学习。并且，对于回归问题，其 loss $ \{ \frac{1}{2}(y - F(x))^2 \} $ 的负梯度与残差也是相同的。所以我们可以使用 loss Function 的负梯度作为残差学习的目标值，此时这个负梯度称为伪残差。下面就是 GBDT 的训练过程简介。

学习过程

1. 初始化模型： $ \{ f\_0(x) = \mathop{\arg\min}\_\gamma \sum\_{i=1}^N L(y\_i, \gamma) \} $
2. 迭代生成 M 个基学习器

    (a). 计算伪残差：
$$
r_{im} = - \left[ \frac{\partial{L(y_i,f(x_i))}}{\partial{f(x_i)}} \right]_{f=f_{m-1}}, i = 1, 2, \cdots, n
$$

    (b). 基于 $ \{ \\{(x\_i, r\_{im})\\}\_{i=1}^n \} $ 生成学习器 $ \{ h\_m(x) \} $

    (c). 计算最优的 $ \{ \gamma\_m \} $
$$
\gamma_m = \mathop{\arg\min}_\gamma \sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \gamma{h_m(x)})
$$

    (d). 更新模型 $ \{ F\_m(x) = F\_{m-1}(x) + \gamma{h\_m(x)} \} $

Boosting 的好处就是每一步都会增加分类出错的样本的权重，而对分类正确的样本的权重降低，从而后面的树就更加关注对错分的样本的训练。

**调参**

1. 树的个数 100 - 10000
2. 树的深度 3 ~ 8
3. 学习速率 0.01 ~ 1
4. 叶子上最大节点数 20
5. 训练采样比例 0.5 ~ 1
6. 训练特征采样比例 $ \{ \sqrt{M} \} $

**优缺点**

优点：

1. 精度高
2. 能处理非线性数据
3. 能够处理多特征类型
4. 适合低维稠密特征

缺点：

1. 难以并行（因为上下两棵树有联系）
2. 多分类时复杂度很大


## 贝叶斯分类器

## 降维方法

## 特征选择与稀疏学习

## 数学概念






